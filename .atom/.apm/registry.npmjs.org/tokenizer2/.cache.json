{"_id":"tokenizer2","_rev":"7-0bb8d19404476dee55f5943f5f951259","name":"tokenizer2","description":"tokenize any text stream given some basic regex rules to match tokens","dist-tags":{"latest":"2.0.0"},"versions":{"1.0.0":{"name":"tokenizer2","version":"1.0.0","description":"tokenize any text stream given some basic regex rules to match tokens","main":"index.js","files":["index.js"],"scripts":{"test":"node tests.js | tap-min"},"repository":{"type":"git","url":"git+https://github.com/smallhelm/tokenizer2.git"},"keywords":["tokenizer","through","stream"],"author":{"name":"smallhelm"},"license":"MIT","bugs":{"url":"https://github.com/smallhelm/tokenizer2/issues"},"homepage":"https://github.com/smallhelm/tokenizer2#readme","dependencies":{"through2":"^2.0.0"},"devDependencies":{"tap-min":"^1.0.0","tape":"^4.0.0"},"gitHead":"a1cc534be82fa8ff6cbf04fad89bfcbcc7eed3a2","_id":"tokenizer2@1.0.0","_shasum":"e8495a8c44c16191a154d80faf839ec6d6cd3dee","_from":".","_npmVersion":"2.9.0","_nodeVersion":"0.10.38","_npmUser":{"name":"smallhelm","email":"dev@smallhelm.com"},"dist":{"shasum":"e8495a8c44c16191a154d80faf839ec6d6cd3dee","tarball":"https://registry.npmjs.org/tokenizer2/-/tokenizer2-1.0.0.tgz"},"maintainers":[{"name":"smallhelm","email":"dev@smallhelm.com"}],"directories":{}},"1.0.1":{"name":"tokenizer2","version":"1.0.1","description":"tokenize any text stream given some basic regex rules to match tokens","main":"index.js","files":["index.js"],"scripts":{"test":"node tests.js | tap-min"},"repository":{"type":"git","url":"git+https://github.com/smallhelm/tokenizer2.git"},"keywords":["tokenizer","through","stream"],"author":{"name":"smallhelm"},"license":"MIT","bugs":{"url":"https://github.com/smallhelm/tokenizer2/issues"},"homepage":"https://github.com/smallhelm/tokenizer2#readme","dependencies":{"through2":"^2.0.0"},"devDependencies":{"tap-min":"^1.0.0","tape":"^4.0.0"},"gitHead":"bd58afdbedbea46ab182ae8332099e75413b27a0","_id":"tokenizer2@1.0.1","_shasum":"c615bb74c13412a469392f2a297cb9a0180d1ec4","_from":".","_npmVersion":"2.9.0","_nodeVersion":"0.10.38","_npmUser":{"name":"smallhelm","email":"dev@smallhelm.com"},"dist":{"shasum":"c615bb74c13412a469392f2a297cb9a0180d1ec4","tarball":"https://registry.npmjs.org/tokenizer2/-/tokenizer2-1.0.1.tgz"},"maintainers":[{"name":"smallhelm","email":"dev@smallhelm.com"}],"directories":{}},"1.1.0":{"name":"tokenizer2","version":"1.1.0","description":"tokenize any text stream given some basic regex rules to match tokens","main":"index.js","files":["index.js"],"scripts":{"test":"node tests.js | tap-min"},"repository":{"type":"git","url":"git+https://github.com/smallhelm/tokenizer2.git"},"keywords":["tokenizer","through","stream"],"author":{"name":"smallhelm"},"license":"MIT","bugs":{"url":"https://github.com/smallhelm/tokenizer2/issues"},"homepage":"https://github.com/smallhelm/tokenizer2#readme","dependencies":{"through2":"^2.0.0"},"devDependencies":{"tap-min":"^1.0.0","tape":"^4.0.0"},"gitHead":"1aa83ea1ec77d0eef54dce19014554a21a124710","_id":"tokenizer2@1.1.0","_shasum":"463be14427b90df0f2a7d655a24335b4eb563cb8","_from":".","_npmVersion":"3.3.8","_nodeVersion":"4.2.1","_npmUser":{"name":"smallhelm","email":"dev@smallhelm.com"},"dist":{"shasum":"463be14427b90df0f2a7d655a24335b4eb563cb8","tarball":"https://registry.npmjs.org/tokenizer2/-/tokenizer2-1.1.0.tgz"},"maintainers":[{"name":"smallhelm","email":"dev@smallhelm.com"}],"directories":{}},"1.2.0":{"name":"tokenizer2","version":"1.2.0","description":"tokenize any text stream given some basic regex rules to match tokens","main":"index.js","files":["core.js","index.js"],"scripts":{"test":"node tests.js | tap-min"},"repository":{"type":"git","url":"git+https://github.com/smallhelm/tokenizer2.git"},"keywords":["tokenizer","through","stream"],"author":{"name":"smallhelm"},"license":"MIT","bugs":{"url":"https://github.com/smallhelm/tokenizer2/issues"},"homepage":"https://github.com/smallhelm/tokenizer2#readme","dependencies":{"through2":"^2.0.0"},"devDependencies":{"tap-min":"^1.0.0","tape":"^4.0.0"},"gitHead":"604cac7e2268756c664d63f6e0650c21cc6dfa43","_id":"tokenizer2@1.2.0","_shasum":"8894625362b913b279fbd0240f9f1a052e02177d","_from":".","_npmVersion":"3.3.8","_nodeVersion":"4.2.1","_npmUser":{"name":"smallhelm","email":"dev@smallhelm.com"},"dist":{"shasum":"8894625362b913b279fbd0240f9f1a052e02177d","tarball":"https://registry.npmjs.org/tokenizer2/-/tokenizer2-1.2.0.tgz"},"maintainers":[{"name":"smallhelm","email":"dev@smallhelm.com"}],"directories":{}},"1.2.1":{"name":"tokenizer2","version":"1.2.1","description":"tokenize any text stream given some basic regex rules to match tokens","main":"index.js","files":["core.js","index.js"],"scripts":{"test":"node tests.js | tap-min"},"repository":{"type":"git","url":"git+https://github.com/smallhelm/tokenizer2.git"},"keywords":["tokenizer","through","stream"],"author":{"name":"smallhelm"},"license":"MIT","bugs":{"url":"https://github.com/smallhelm/tokenizer2/issues"},"homepage":"https://github.com/smallhelm/tokenizer2#readme","dependencies":{"through2":"^2.0.0"},"devDependencies":{"tap-min":"^1.0.0","tape":"^4.0.0"},"gitHead":"05c074d8c01fb19cc7a4fcdb90ead63c06d71a62","_id":"tokenizer2@1.2.1","_shasum":"787c882d1fd1a62e2af3591218ff4cd85b86b0e0","_from":".","_npmVersion":"2.14.20","_nodeVersion":"4.4.0","_npmUser":{"name":"smallhelm","email":"dev@smallhelm.com"},"dist":{"shasum":"787c882d1fd1a62e2af3591218ff4cd85b86b0e0","tarball":"https://registry.npmjs.org/tokenizer2/-/tokenizer2-1.2.1.tgz"},"maintainers":[{"name":"smallhelm","email":"dev@smallhelm.com"}],"_npmOperationalInternal":{"host":"packages-12-west.internal.npmjs.com","tmp":"tmp/tokenizer2-1.2.1.tgz_1458359623817_0.5964990858919919"},"directories":{}},"2.0.0":{"name":"tokenizer2","version":"2.0.0","description":"tokenize any text stream given some basic regex rules to match tokens","main":"index.js","files":["core.js","index.js"],"scripts":{"test":"node tests.js | tap-min"},"repository":{"type":"git","url":"git+https://github.com/smallhelm/tokenizer2.git"},"keywords":["tokenizer","through","stream"],"author":{"name":"smallhelm"},"license":"MIT","bugs":{"url":"https://github.com/smallhelm/tokenizer2/issues"},"homepage":"https://github.com/smallhelm/tokenizer2#readme","dependencies":{"through2":"^2.0.0"},"devDependencies":{"tap-min":"^1.0.0","tape":"^4.0.0"},"gitHead":"1df6103ee6a24696fab477017be3a9b0760ca72f","_id":"tokenizer2@2.0.0","_shasum":"8e93f6142bbc9d1e9b68e29293517187b9833b5f","_from":".","_npmVersion":"2.14.20","_nodeVersion":"4.4.0","_npmUser":{"name":"smallhelm","email":"dev@smallhelm.com"},"dist":{"shasum":"8e93f6142bbc9d1e9b68e29293517187b9833b5f","tarball":"https://registry.npmjs.org/tokenizer2/-/tokenizer2-2.0.0.tgz"},"maintainers":[{"name":"smallhelm","email":"dev@smallhelm.com"}],"_npmOperationalInternal":{"host":"packages-13-west.internal.npmjs.com","tmp":"tmp/tokenizer2-2.0.0.tgz_1458404689781_0.5975087643601"},"directories":{}}},"readme":"# tokenizer2\n\n[![build status](https://secure.travis-ci.org/smallhelm/tokenizer2.png)](https://travis-ci.org/smallhelm/tokenizer2)\n[![dependency status](https://david-dm.org/smallhelm/tokenizer2.svg)](https://david-dm.org/smallhelm/tokenizer2)\n\ntokenize any text stream given some basic regex rules to match tokens\n\n## Example\n```js\nvar tokenizer2 = require('tokenizer2');\n\n//create a readable/writeable stream\nvar token_stream = tokenizer2();\n\n//make some rules\ntoken_stream.addRule(/^[\\s]+$/               , 'whitespace');\ntoken_stream.addRule(/^\"([^\"]|\\\\\")*\"$/       , 'string');\ntoken_stream.addRule(/^[-+]?[0-9]+\\.?[0-9]*$/, 'number');\ntoken_stream.addRule(/^[^\"0-9\\s][^\\s]*$/     , 'symbol');\n\n//write some info to the console\ntoken_stream.on('data', function(token){\n  console.log('token:', token);\n});\ntoken_stream.on('end', function(){\n  console.log('DONE');\n});\n\n//pipe in some data\nfs.createReadStream('./demo.txt').pipe(token_stream);\n```\ndemo.txt\n```txt\nprint \"some multi-\nlined string\"\n\n123.25 times -10\n```\nThe output\n```js\ntoken: {type: 'symbol'    , src: 'print',  line: 1, col:  1 }\ntoken: {type: 'whitespace', src: ' ',      line: 1, col:  6 }\ntoken: {type: 'string'    , src: '\"some multi-\\nlined string\"', line: 1, col: 7 }\ntoken: {type: 'whitespace', src: '\\n\\n',   line: 2, col: 14 }\ntoken: {type: 'number'    , src: '123.25', line: 4, col:  1 }\ntoken: {type: 'whitespace', src: ' ',      line: 4, col:  7 }\ntoken: {type: 'symbol'    , src: 'times',  line: 4, col:  8 }\ntoken: {type: 'whitespace', src: ' ',      line: 4, col: 13 }\ntoken: {type: 'number'    , src: '-10',    line: 4, col: 14 }\ntoken: {type: 'whitespace', src: '\\n',     line: 4, col: 17 }\nDONE\n```\n\n### What if more than one rule matches a token? \n\n`token_stream.addRule` adds rules in an order sensitive way. The first matching rule will be used.\n\n### Why tokenizer2\n\nThe key difference between this and [tokenizer](https://github.com/Floby/node-tokenizer) is the way it matches rules. `tokenizer` uses [disect](https://github.com/Floby/node-disect) to do bisection on a chunk of text. This is a fast approach, however doesn't work well if your regex rule expects some specific characters at the end of the token. To solve this tokenizer2 simply starts at the beginning of the chunk, and finds the longest matching rule.\n\nOther differences\n * tokenizer2 wraps [through2.obj](https://www.npmjs.com/package/through2) so all the node stream APIs should work nicely\n * tokenizer2 uses the standard `'data'` event to emit the tokens\n * tokenizer2 emits line and col numbers\n\n## Non-streaming, synchronous API\n\nIf, for whatever reason, you don't want to use the streaming api. There is a lighter weight, synchronous api.\n\n```js\nvar core = require('tokenizer2/core');\n\nvar t = core(function(token){\n  //called synchronously on every token found\n});\n\n\n//add rules just like the streaming api\nt.addRule(/^[\\s]+$/, 'whitespace');\n\n//Give it strings to tokenize\nt.onText(\"some text to tokenize\");\nt.onText(\"some more text\");\n\n//Call this when it's done\nt.end();//this may throw an error\n```\n\n## License\nMIT\n","maintainers":[{"name":"farskipper","email":"farskipper@smallhelm.com"},{"name":"smallhelm","email":"dev@smallhelm.com"}],"time":{"modified":"2016-03-24T19:58:54.078Z","created":"2015-06-17T16:30:52.219Z","1.0.0":"2015-06-17T16:30:52.219Z","1.0.1":"2015-06-17T16:55:28.073Z","1.1.0":"2015-11-03T13:52:35.881Z","1.2.0":"2015-11-14T14:54:23.681Z","1.2.1":"2016-03-19T03:53:46.235Z","2.0.0":"2016-03-19T16:24:52.431Z"},"homepage":"https://github.com/smallhelm/tokenizer2#readme","keywords":["tokenizer","through","stream"],"repository":{"type":"git","url":"git+https://github.com/smallhelm/tokenizer2.git"},"author":{"name":"smallhelm"},"bugs":{"url":"https://github.com/smallhelm/tokenizer2/issues"},"license":"MIT","readmeFilename":"README.md","_attachments":{},"_etag":"\"B55PGNG11U262VDL3YQF1CB1X\""}